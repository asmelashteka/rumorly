"""
Reproducing
Zhao, Zhe, Paul Resnick, and Qiaozhu Mei. "Enquiring minds: Early
detection of rumors in social media from enquiry posts." Proceedings of
the 24th International Conference on World Wide Web. ACM, 2015.
http://dl.acm.org/citation.cfm?id=2741637
"""

import sys
import time
import re
import json
import gzip
from collections import Counter
import random
from random import randint
import matplotlib.pyplot as plt
import numpy as np
from datasketch import MinHash, MinHashLSH
import networkx as nx
import twitter
from twitter import STREAMING_API
#from training import train_classifier


non_bmp_map = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)
lsh_signal=MinHashLSH(threshold=0.6, num_perm=50)
lsh_non_signal=MinHashLSH(threshold=0.6, num_perm=50)
g=nx.Graph()
signal_tweets=[]
non_signal_tweets=[]
signal_id_text={}
non_signal_id_text={}
signal_minhashes={}
non_signal_minhashes={}

reg_ex=re.compile(r"(is\s?(that\s?|this\s?|it\s?)true\s?[?]?)|\breal\b|reall[y?]*|unconfirmed|\brumor\b|debunk|(this\s?|that\s?|it\s?)is\s?not\s?true|\bwha[t?!]+\b")

def is_signal_tweet(tweet_text):
	"""
	identifies a tweet as signal or not using the following RegEx:
	is (that | this | it) true
	wh[a]*t[?!][?1]*
	( real? | really ? | unconfirmed )
	(rumor | debunk)
	(that | this | it) is not true

	Args:
	@param(str): input tweet text
	@output: true if the tweet is a signal tweet, false otherwise
	"""

	if reg_ex.search(tweet_text):
		return True
def minhash(tweet_text,tweet_id,lsh_index,id_text_dict):
	"""
	Generate a minhash from tweet_text and appends it to dictionary with text as key and appends to lsh index
	Args:
	param1(str): "text" part of the tweet
	param2(dictionary):Dictionary containing the {tweet_text:minhash(tweet_text)} items
	param3(lsh index):LSH Dictionary optimised for a particular treshold which accepts minhash objects
	Returns:
	Minhash value of the tweettext and appends it to the lsh index and minhashes dictionary.
	"""
	words = tweet_text.split(" ")
	trigrams=[]
	for i in range(len(words)-2):
		trigrams.append(words[i]+words[i+1]+words[i+2])
	m=MinHash(num_perm=50)
	for d in trigrams:
		m.update(d.encode('UTF-8'))
	id_text_dict.update({tweet_id:tweet_text})
	lsh_index.insert(tweet_id,m)
	return m

def generate_undirected_graph(tweet_id,min_hash):
	"""
	An undirected graph of tweets is built by including an edge joining
	any tweet pair with a jaccard similarity of 0.6. Minhash is used to
	efficiently compute the jaccard similarity. The connected components
	in this graph are the clusters.

	Args:
	param(dictionary): Dictionary containing {text:minhash} items

	Returns:
	Graph, where nodes are tweet_texts and edges are formed between texts if they are similar by value greater than threshold
	"""
	g.add_node(tweet_id)
	similar=lsh_signal.query(min_hash)
	for each in similar:
		g.add_edge(tweet_id,each)


def connected_components(g):
	"""
	From the undirected graph generated by the functiongen_undirected_graph(),
	the graph is filtered to include only the connected components that include more than the threshold number of nodes

	Args:
	param(graph): Graph containing clusters of connected components

	Returns:
	Clusters containing more than threshold no of tweets
	"""

	conn_comp=sorted(nx.connected_components(g),key=len,reverse=True)
	req_conn_comp=[]
	for each_cluster in conn_comp:
		if (len(each_cluster)>3):
			req_conn_comp.append(each_cluster)
		else:
			pass
	return req_conn_comp

def extract_summary(list_signal_tweets):
	"""
	For each cluster extracts statement that summarizes the tweets in a signal cluster
	Args:
	param: Set of tweet ids

	Returns:
	Most frequent and continuous substrings (3grams that
	appear in more than 80% of the tweets) in order.
	"""

	no_of_tweets=len(list_signal_tweets)
	req_cutoff=0.8*no_of_tweets
	words_list=[]
	shinglesincluster =[]
	for each_tweet in list_signal_tweets:
		sente=(each_tweet.translate(non_bmp_map))
		words = sente.split(" ")
		for index in range(0, len(words) - 2):
			shingle = words[index] + " " + words[index + 1] + " " + words[index + 2]
			shinglesincluster.append(shingle)
	tot=dict(Counter(shinglesincluster))
	for k,v in tot.items():
		if v>=req_cutoff:
			words_list.append(k)
	sentence= ' '.join(words_list)
	return sentence


def match_non_signal_tweets(sentence):
	"""
	From a sentence that summarizes each cluster,the sentence is matched against
	nogsignal tweet_texts to match the tweets that belong to the cluster but donot contain signal patterns
	Args:
	param(str):Statement that summarizes each cluster
	Returns:
	Tweet_texts that belong to the cluster
	"""

	words =sentence.split(" ")
	m=MinHash(num_perm=50)
	trigrams=[]
	for i in range(len(words)-2):
		trigrams.append(words[i]+words[i+1]+words[i+2])
	for d in trigrams:
		m.update(d.encode('UTF-8'))
	sim_non_sig_tweets=lsh_non_signal.query(m)
	return sim_non_sig_tweets




def gen_stream(fin=None):
	#TODO:consumer producer with queue
	stream  = twitter.STREAMING_API(key=1, payload={})
	for tweet in stream.run():
		tweet_lang=tweet.get('lang')
		tweet_text=tweet.get('text')
		if tweet_lang in ('en','en-gb'):
			if len(tweet_text.split())>3:
				yield tweet


def pipeline():
	for tweet in gen_stream():
		tweet_id=tweet.get('id')
		tweet_text=tweet.get('text')
		if(is_signal_tweet(tweet_text)):
			m=minhash(tweet_text,tweet_id,lsh_signal,signal_id_text)
			gen_undirected_graph(tweet_id,m)
		else:
			minhash(tweet_text,tweet_id,lsh_non_signal,non_signal_id_text)
	rumor_ids=connected_components(g)
	for each_cluster in rumor_ids:
		sig_tweet_texts=[]
		sig_tweets_clus=[]
		non_signal_tweets_clus=[]
		for each_id in each_cluster:
			sig_tweet_texts.append(signal_id_text[each_id])
		sentence=extract_summary(sig_tweet_texts)
		sim_non_sig_tweets=match_non_signal_tweets(sentence)


if __name__ == '__main__':
	pipeline()
